{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantize Wan2.2-Animate-14B DiT to NF4 (4-bit)\n",
    "\n",
    "**Run this once on a Kaggle GPU (T4) session.**\n",
    "\n",
    "This notebook:\n",
    "1. Downloads the Wan2.2 code repo and the full-precision DiT model from HuggingFace\n",
    "2. Quantizes all Linear layers to NF4 using bitsandbytes\n",
    "3. Saves the quantized model (~9 GB)\n",
    "\n",
    "After running, create a Kaggle dataset from the output for use in inference notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install dependencies & clone repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kaggle already has: torch, torchvision, numpy, opencv, tqdm, PIL\n",
    "# Install/upgrade what's needed with pinned versions from Wan2.2:\n",
    "!pip install -q \\\n",
    "    bitsandbytes>=0.43.0 \\\n",
    "    huggingface_hub \\\n",
    "    \"transformers>=4.49.0,<=4.51.3\" \\\n",
    "    \"diffusers>=0.31.0\" \\\n",
    "    \"accelerate>=1.1.1\" \\\n",
    "    easydict ftfy regex decord peft einops safetensors sentencepiece\n",
    "\n",
    "# Clone the Wan2.2 repo (needed for WanAnimateModel class definition)\n",
    "!git clone https://github.com/Wan-Video/Wan2.2.git /kaggle/working/Wan2.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Download DiT model from HuggingFace\n",
    "\n",
    "We only download the DiT files (config.json + safetensors shards + index).\n",
    "Skip T5, CLIP, VAE — they're not needed for quantization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "REPO_ID = \"Wan-AI/Wan2.2-Animate-14B\"\n",
    "TMP_MODEL_DIR = \"/tmp/Wan2.2-Animate-14B\"       # full-precision shards go here (tmpfs, not persistent disk)\n",
    "OUTPUT_DIR = \"/kaggle/working/Wan2.2-Animate-14B-NF4\"  # quantized model goes here (persistent)\n",
    "\n",
    "os.makedirs(TMP_MODEL_DIR, exist_ok=True)\n",
    "\n",
    "# Only the DiT files — skip T5 (~11.4 GB), CLIP (~4.8 GB), VAE (~0.5 GB)\n",
    "dit_files = [\n",
    "    \"config.json\",\n",
    "    \"diffusion_pytorch_model.safetensors.index.json\",\n",
    "    \"diffusion_pytorch_model-00001-of-00004.safetensors\",\n",
    "    \"diffusion_pytorch_model-00002-of-00004.safetensors\",\n",
    "    \"diffusion_pytorch_model-00003-of-00004.safetensors\",\n",
    "    \"diffusion_pytorch_model-00004-of-00004.safetensors\",\n",
    "]\n",
    "\n",
    "for filename in dit_files:\n",
    "    dest = os.path.join(TMP_MODEL_DIR, filename)\n",
    "    if os.path.exists(dest):\n",
    "        print(f\"Already exists: {filename}\")\n",
    "        continue\n",
    "    print(f\"Downloading: {filename}...\")\n",
    "    hf_hub_download(\n",
    "        repo_id=REPO_ID,\n",
    "        filename=filename,\n",
    "        local_dir=TMP_MODEL_DIR,\n",
    "    )\n",
    "    print(f\"  Done: {os.path.getsize(dest) / 1e9:.2f} GB\")\n",
    "\n",
    "print(\"\\nModel directory contents:\")\n",
    "for f in sorted(os.listdir(TMP_MODEL_DIR)):\n",
    "    size = os.path.getsize(os.path.join(TMP_MODEL_DIR, f))\n",
    "    print(f\"  {f:60s} {size / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Setup imports & check GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "from diffusers import BitsAndBytesConfig\n",
    "\n",
    "# Add the cloned Wan2.2 repo to path\n",
    "WAN_REPO_PATH = \"/kaggle/working/Wan2.2\"\n",
    "if WAN_REPO_PATH not in sys.path:\n",
    "    sys.path.insert(0, WAN_REPO_PATH)\n",
    "\n",
    "from wan.modules.animate.model_animate import WanAnimateModel\n",
    "\n",
    "print(f\"torch: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)} ({torch.cuda.get_device_properties(i).total_mem / 1e9:.1f} GB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load and quantize the DiT\n",
    "\n",
    "`from_pretrained` with `device_map=\"auto\"` loads safetensors shards one at a time,\n",
    "quantizes each parameter on the fly, and places it on GPU(s).\n",
    "\n",
    "- **Peak RAM:** ~12 GB (one shard at a time)\n",
    "- **Peak VRAM:** ~9 GB (accumulated quantized parameters)\n",
    "- **Time:** ~5-10 minutes on T4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,  # quantize the scale factors too, saves ~0.4 GB\n",
    ")\n",
    "\n",
    "print(\"Loading and quantizing DiT...\")\n",
    "model = WanAnimateModel.from_pretrained(\n",
    "    TMP_MODEL_DIR,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "print(\"Done!\")\n",
    "\n",
    "# Clean up full-precision shards from /tmp — no longer needed\n",
    "print(\"Cleaning up full-precision shards from /tmp...\")\n",
    "shutil.rmtree(TMP_MODEL_DIR)\n",
    "print(\"Freed ~34.5 GB from /tmp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Inspect the quantized model\n",
    "\n",
    "Verify that Linear layers were replaced with Linear4bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bitsandbytes as bnb\n",
    "\n",
    "total_params = 0\n",
    "quantized_params = 0\n",
    "full_precision_params = 0\n",
    "\n",
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, bnb.nn.Linear4bit):\n",
    "        n = module.in_features * module.out_features\n",
    "        quantized_params += n\n",
    "        total_params += n\n",
    "    elif isinstance(module, torch.nn.Linear):\n",
    "        n = module.in_features * module.out_features\n",
    "        full_precision_params += n\n",
    "        total_params += n\n",
    "\n",
    "print(f\"Total Linear params:          {total_params / 1e9:.2f} B\")\n",
    "print(f\"Quantized (NF4):              {quantized_params / 1e9:.2f} B ({100 * quantized_params / total_params:.1f}%)\")\n",
    "print(f\"Full precision (not touched):  {full_precision_params / 1e9:.2f} B ({100 * full_precision_params / total_params:.1f}%)\")\n",
    "print()\n",
    "\n",
    "# Show VRAM usage\n",
    "if torch.cuda.is_available():\n",
    "    allocated = torch.cuda.memory_allocated() / 1e9\n",
    "    reserved = torch.cuda.memory_reserved() / 1e9\n",
    "    print(f\"VRAM allocated: {allocated:.1f} GB\")\n",
    "    print(f\"VRAM reserved:  {reserved:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Save quantized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Saving quantized model to {OUTPUT_DIR}...\")\n",
    "model.save_pretrained(OUTPUT_DIR)\n",
    "print(\"Saved!\")\n",
    "\n",
    "# Report output size\n",
    "total_bytes = sum(\n",
    "    os.path.getsize(os.path.join(root, f))\n",
    "    for root, _, files in os.walk(OUTPUT_DIR)\n",
    "    for f in files\n",
    ")\n",
    "print(f\"\\nQuantized model size: {total_bytes / 1e9:.1f} GB\")\n",
    "print(f\"\\nOutput files:\")\n",
    "for f in sorted(os.listdir(OUTPUT_DIR)):\n",
    "    size = os.path.getsize(os.path.join(OUTPUT_DIR, f))\n",
    "    print(f\"  {f:60s} {size / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Verify: reload the quantized model\n",
    "\n",
    "Quick sanity check — load from the saved directory to confirm it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free the current model\n",
    "del model\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Reload from saved quantized checkpoint\n",
    "print(\"Reloading quantized model from disk...\")\n",
    "model_reloaded = WanAnimateModel.from_pretrained(\n",
    "    OUTPUT_DIR,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "print(\"Reload successful!\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    allocated = torch.cuda.memory_allocated() / 1e9\n",
    "    print(f\"VRAM after reload: {allocated:.1f} GB\")\n",
    "\n",
    "del model_reloaded\n",
    "torch.cuda.empty_cache()\n",
    "print(\"\\nDone. Create a Kaggle dataset from the output directory for inference.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Push to HuggingFace private repo\n",
    "\n",
    "Upload the quantized model so you can pull it in any future session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi, login\n",
    "\n",
    "# Login — on Kaggle, add your HF token as a Kaggle secret named \"HF_TOKEN\"\n",
    "# Or paste it directly: login(token=\"hf_...\")\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "secrets = UserSecretsClient()\n",
    "login(token=secrets.get_secret(\"HF_TOKEN\"))\n",
    "\n",
    "# Change this to your HF username\n",
    "HF_USERNAME = \"YOUR_USERNAME\"\n",
    "HF_REPO = f\"{HF_USERNAME}/Wan2.2-Animate-14B-NF4\"\n",
    "\n",
    "api = HfApi()\n",
    "api.create_repo(HF_REPO, private=True, exist_ok=True)\n",
    "\n",
    "print(f\"Uploading quantized model to {HF_REPO} (private)...\")\n",
    "api.upload_folder(\n",
    "    folder_path=OUTPUT_DIR,\n",
    "    repo_id=HF_REPO,\n",
    "    repo_type=\"model\",\n",
    ")\n",
    "print(f\"Done! Model available at: https://huggingface.co/{HF_REPO}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "In your inference notebook, pull the quantized model:\n",
    "```python\n",
    "from huggingface_hub import snapshot_download\n",
    "snapshot_download(\"YOUR_USERNAME/Wan2.2-Animate-14B-NF4\", local_dir=\"/tmp/dit-nf4\")\n",
    "\n",
    "model = WanAnimateModel.from_pretrained(\"/tmp/dit-nf4\", device_map=\"auto\", torch_dtype=torch.float16)\n",
    "```\n",
    "The saved `config.json` contains the quantization config — no need to re-specify `BitsAndBytesConfig`.\n",
    "\n",
    "LoRA works on quantized layers as usual (QLoRA pattern)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
